{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heart disease is a leading cause of death worldwide, and early detection and prevention can save lives. In this project, we analysed data from a 2020 annual CDC survey data of 400k adults related to their health status, and applied different machine learning algorithms aiming to predict chance of one individual having heart disease. The project first performed data cleaning and feature selection to prepare the data for modeling. Then, several machine learning models such as LDA, Random Forest, SVM, Decision Tree, XGBoost, and Neural Network were applied to find the model with the highest sensitivity for the final prediction. The results showed that the XGBoost model performed the best in terms of sensitivity, and thus was selected as the final model for the prediction of heart disease. We created a user interface and linked with our XGBoost model, which allow user to input data about their personal health status and returns whether the patient require further diagnoise or not. This project highlights the importance of data preparation and model selection in achieving accurate results in the field of medical diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"heart_2020_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 319795 entries, 0 to 319794\n",
      "Data columns (total 18 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   HeartDisease      319795 non-null  object \n",
      " 1   BMI               319795 non-null  float64\n",
      " 2   Smoking           319795 non-null  object \n",
      " 3   AlcoholDrinking   319795 non-null  object \n",
      " 4   Stroke            319795 non-null  object \n",
      " 5   PhysicalHealth    319795 non-null  float64\n",
      " 6   MentalHealth      319795 non-null  float64\n",
      " 7   DiffWalking       319795 non-null  object \n",
      " 8   Sex               319795 non-null  object \n",
      " 9   AgeCategory       319795 non-null  object \n",
      " 10  Race              319795 non-null  object \n",
      " 11  Diabetic          319795 non-null  object \n",
      " 12  PhysicalActivity  319795 non-null  object \n",
      " 13  GenHealth         319795 non-null  object \n",
      " 14  SleepTime         319795 non-null  float64\n",
      " 15  Asthma            319795 non-null  object \n",
      " 16  KidneyDisease     319795 non-null  object \n",
      " 17  SkinCancer        319795 non-null  object \n",
      "dtypes: float64(4), object(14)\n",
      "memory usage: 43.9+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319795, 18)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data is cleaned, there is no necessary to check for NA values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of variables\n",
    "**Number of unique values for each variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeartDisease           2\n",
       "BMI                 3604\n",
       "Smoking                2\n",
       "AlcoholDrinking        2\n",
       "Stroke                 2\n",
       "PhysicalHealth        31\n",
       "MentalHealth          31\n",
       "DiffWalking            2\n",
       "Sex                    2\n",
       "AgeCategory           13\n",
       "Race                   6\n",
       "Diabetic               4\n",
       "PhysicalActivity       2\n",
       "GenHealth              5\n",
       "SleepTime             24\n",
       "Asthma                 2\n",
       "KidneyDisease          2\n",
       "SkinCancer             2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeartDisease ['No' 'Yes'] \n",
      "\n",
      "BMI [16.6  20.34 26.58 ... 62.42 51.46 46.56] \n",
      "\n",
      "Smoking ['Yes' 'No'] \n",
      "\n",
      "AlcoholDrinking ['No' 'Yes'] \n",
      "\n",
      "Stroke ['No' 'Yes'] \n",
      "\n",
      "PhysicalHealth [ 3.  0. 20. 28.  6. 15.  5. 30.  7.  1.  2. 21.  4. 10. 14. 18.  8. 25.\n",
      " 16. 29. 27. 17. 24. 12. 23. 26. 22. 19.  9. 13. 11.] \n",
      "\n",
      "MentalHealth [30.  0.  2.  5. 15.  8.  4.  3. 10. 14. 20.  1.  7. 24.  9. 28. 16. 12.\n",
      "  6. 25. 17. 18. 21. 29. 22. 13. 23. 27. 26. 11. 19.] \n",
      "\n",
      "DiffWalking ['No' 'Yes'] \n",
      "\n",
      "Sex ['Female' 'Male'] \n",
      "\n",
      "AgeCategory ['55-59' '80 or older' '65-69' '75-79' '40-44' '70-74' '60-64' '50-54'\n",
      " '45-49' '18-24' '35-39' '30-34' '25-29'] \n",
      "\n",
      "Race ['White' 'Black' 'Asian' 'American Indian/Alaskan Native' 'Other'\n",
      " 'Hispanic'] \n",
      "\n",
      "Diabetic ['Yes' 'No' 'No, borderline diabetes' 'Yes (during pregnancy)'] \n",
      "\n",
      "PhysicalActivity ['Yes' 'No'] \n",
      "\n",
      "GenHealth ['Very good' 'Fair' 'Good' 'Poor' 'Excellent'] \n",
      "\n",
      "SleepTime [ 5.  7.  8.  6. 12.  4.  9. 10. 15.  3.  2.  1. 16. 18. 14. 20. 11. 13.\n",
      " 17. 24. 19. 21. 22. 23.] \n",
      "\n",
      "Asthma ['Yes' 'No'] \n",
      "\n",
      "KidneyDisease ['No' 'Yes'] \n",
      "\n",
      "SkinCancer ['Yes' 'No'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for feature in data.columns:\n",
    "    print(feature, data[feature].unique(),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Body Mass Index (BMI) values is numeric continuous data.\n",
    "\n",
    "The following predictors are binary: \"HeartDisease\", \"Smoking\", \"AlcoholDrinking\", \"Stroke\", \"DiffWalking\", \"PhysicalActivity\", \"Asthma\", \"KidneyDisease\", and \"SkinCancer\". There are only 2 unique values for each, which can be factored into 1 and 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The physical health predictor is a numeric continuous variable providing information about the amount of days of injuries for individual participants over the past 30 days. The larger the value, the poorer physical condition of participant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to mental health predictor, which indicates the number of days in the past 30 days that the participant does NOT feel well. The larger value indicating a worse mental health status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding all variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert binary to 1/0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_binary = data[data.columns].replace({'Yes':1, 'No':0, 'Male':1,'Female':0,'No, borderline diabetes':'0','Yes (during pregnancy)':'1' })\n",
    "data_binary['Diabetic'] = data_binary['Diabetic'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HeartDisease</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>AlcoholDrinking</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>PhysicalHealth</th>\n",
       "      <th>MentalHealth</th>\n",
       "      <th>DiffWalking</th>\n",
       "      <th>Sex</th>\n",
       "      <th>AgeCategory</th>\n",
       "      <th>Race</th>\n",
       "      <th>Diabetic</th>\n",
       "      <th>PhysicalActivity</th>\n",
       "      <th>GenHealth</th>\n",
       "      <th>SleepTime</th>\n",
       "      <th>Asthma</th>\n",
       "      <th>KidneyDisease</th>\n",
       "      <th>SkinCancer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>16.60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55-59</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Very good</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20.34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80 or older</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Very good</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>26.58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>65-69</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Fair</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>24.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75-79</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Good</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>23.71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40-44</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Very good</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HeartDisease    BMI  Smoking  AlcoholDrinking  Stroke  PhysicalHealth  \\\n",
       "0             0  16.60        1                0       0             3.0   \n",
       "1             0  20.34        0                0       1             0.0   \n",
       "2             0  26.58        1                0       0            20.0   \n",
       "3             0  24.21        0                0       0             0.0   \n",
       "4             0  23.71        0                0       0            28.0   \n",
       "\n",
       "   MentalHealth  DiffWalking  Sex  AgeCategory   Race  Diabetic  \\\n",
       "0          30.0            0    0        55-59  White         1   \n",
       "1           0.0            0    0  80 or older  White         0   \n",
       "2          30.0            0    1        65-69  White         1   \n",
       "3           0.0            0    0        75-79  White         0   \n",
       "4           0.0            1    0        40-44  White         0   \n",
       "\n",
       "   PhysicalActivity  GenHealth  SleepTime  Asthma  KidneyDisease  SkinCancer  \n",
       "0                 1  Very good        5.0       1              0           1  \n",
       "1                 1  Very good        7.0       0              0           0  \n",
       "2                 1       Fair        8.0       1              0           0  \n",
       "3                 0       Good        6.0       0              0           1  \n",
       "4                 1  Very good        8.0       0              0           0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_binary.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert categorial variables to dummy indicator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AgeCategory', 'Race', 'GenHealth']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns = [name for name in data_binary.columns \n",
    "                       if data_binary[name].dtype=='O']\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HeartDisease</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>AlcoholDrinking</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>PhysicalHealth</th>\n",
       "      <th>MentalHealth</th>\n",
       "      <th>DiffWalking</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Diabetic</th>\n",
       "      <th>...</th>\n",
       "      <th>Race_Asian</th>\n",
       "      <th>Race_Black</th>\n",
       "      <th>Race_Hispanic</th>\n",
       "      <th>Race_Other</th>\n",
       "      <th>Race_White</th>\n",
       "      <th>GenHealth_Excellent</th>\n",
       "      <th>GenHealth_Fair</th>\n",
       "      <th>GenHealth_Good</th>\n",
       "      <th>GenHealth_Poor</th>\n",
       "      <th>GenHealth_Very good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>16.60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20.34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>26.58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>24.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>23.71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HeartDisease    BMI  Smoking  AlcoholDrinking  Stroke  PhysicalHealth  \\\n",
       "0             0  16.60        1                0       0             3.0   \n",
       "1             0  20.34        0                0       1             0.0   \n",
       "2             0  26.58        1                0       0            20.0   \n",
       "3             0  24.21        0                0       0             0.0   \n",
       "4             0  23.71        0                0       0            28.0   \n",
       "\n",
       "   MentalHealth  DiffWalking  Sex  Diabetic  ...  Race_Asian  Race_Black  \\\n",
       "0          30.0            0    0         1  ...           0           0   \n",
       "1           0.0            0    0         0  ...           0           0   \n",
       "2          30.0            0    1         1  ...           0           0   \n",
       "3           0.0            0    0         0  ...           0           0   \n",
       "4           0.0            1    0         0  ...           0           0   \n",
       "\n",
       "   Race_Hispanic  Race_Other  Race_White  GenHealth_Excellent  GenHealth_Fair  \\\n",
       "0              0           0           1                    0               0   \n",
       "1              0           0           1                    0               0   \n",
       "2              0           0           1                    0               1   \n",
       "3              0           0           1                    0               0   \n",
       "4              0           0           1                    0               0   \n",
       "\n",
       "   GenHealth_Good  GenHealth_Poor  GenHealth_Very good  \n",
       "0               0               0                    1  \n",
       "1               0               0                    1  \n",
       "2               0               0                    0  \n",
       "3               1               0                    0  \n",
       "4               0               0                    1  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dummy = pd.get_dummies(data=data_binary, columns=categorical_columns, drop_first=False)\n",
    "data_dummy.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting categorical variables into dummy indicators will be later utilized in model prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (255836, 38)\n",
      "y_train shape: (255836,)\n",
      "X_test shape: (63959, 38)\n",
      "y_test shape: (63959,)\n",
      "train_data shape: (255836, 39)\n",
      "test_data shape: (63959, 39)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# assume df is the DataFrame and y is the target variable\n",
    "# split the data into training and test sets with 80% for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_dummy.drop(columns=[data_dummy.columns[0]]), \n",
    "                                                    data_dummy.iloc[:,0], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "train_data = pd.concat([pd.DataFrame(X_train), pd.DataFrame(y_train)], axis=1)\n",
    "test_data = pd.concat([pd.DataFrame(X_test), pd.DataFrame(y_test)], axis=1)\n",
    "\n",
    "# display the shapes of the train and test sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"train_data shape:\", train_data.shape)\n",
    "print(\"test_data shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of response variable - HeartDisease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    234055\n",
       "1     21781\n",
       "Name: HeartDisease, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series.value_counts(train_data.HeartDisease)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is heavily unbalanced with 292422 respondents without heart disease, and only 27373 of participants shown of having heart disease. Having imbalanced dataset might result in a poor performance of the prediction, as the result might be biased toward the majority class indicaiting no heart disease. For our model, we tend to be biased toward \"having heart disease\", so in reality a higher chance to be predicted to have \"heart disease\" to prevent misdiagnose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE Technique\n",
    "Address the issue we have for imbalanced dataset by synthesizing new examples from the minority class. It involves randomly selecting a minority class example and finding its k nearest neighbors in the feature space. New examples are then generated by interpolating between the selected example and its neighbors. This aim to increase the number of examples in minority class.\n",
    "\n",
    "SMOTE technique is perfect approach to deal with imbalance problems for binary variable. Hence can be eperfectly utilized for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (/Users/yutongwu/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BorderlineSMOTE\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_classification\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/__init__.py:52\u001b[0m\n\u001b[1;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m         combine,\n\u001b[1;32m     54\u001b[0m         ensemble,\n\u001b[1;32m     55\u001b[0m         exceptions,\n\u001b[1;32m     56\u001b[0m         metrics,\n\u001b[1;32m     57\u001b[0m         over_sampling,\n\u001b[1;32m     58\u001b[0m         pipeline,\n\u001b[1;32m     59\u001b[0m         tensorflow,\n\u001b[1;32m     60\u001b[0m         under_sampling,\n\u001b[1;32m     61\u001b[0m         utils,\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/combine/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/combine/_smote_enn.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/base.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_sampling_strategy, check_target_type\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArraysTransformer\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSamplerMixin\u001b[39;00m(BaseEstimator, metaclass\u001b[38;5;241m=\u001b[39mABCMeta):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/utils/_param_validation.py:908\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_valid_param  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m--> 908\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    909\u001b[0m     HasMethods,\n\u001b[1;32m    910\u001b[0m     Hidden,\n\u001b[1;32m    911\u001b[0m     Interval,\n\u001b[1;32m    912\u001b[0m     Options,\n\u001b[1;32m    913\u001b[0m     StrOptions,\n\u001b[1;32m    914\u001b[0m     _ArrayLikes,\n\u001b[1;32m    915\u001b[0m     _Booleans,\n\u001b[1;32m    916\u001b[0m     _Callables,\n\u001b[1;32m    917\u001b[0m     _CVObjects,\n\u001b[1;32m    918\u001b[0m     _InstancesOf,\n\u001b[1;32m    919\u001b[0m     _IterablesNotString,\n\u001b[1;32m    920\u001b[0m     _MissingValues,\n\u001b[1;32m    921\u001b[0m     _NoneConstraint,\n\u001b[1;32m    922\u001b[0m     _PandasNAConstraint,\n\u001b[1;32m    923\u001b[0m     _RandomStates,\n\u001b[1;32m    924\u001b[0m     _SparseMatrices,\n\u001b[1;32m    925\u001b[0m     _VerboseHelper,\n\u001b[1;32m    926\u001b[0m     make_constraint,\n\u001b[1;32m    927\u001b[0m     validate_params,\n\u001b[1;32m    928\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (/Users/yutongwu/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py)"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X = X_train\n",
    "y = y_train\n",
    "\n",
    "print('Before SMOTE:', X.shape, y.shape)\n",
    "\n",
    "\n",
    "# # create SMOTE object with desired sampling strategy\n",
    "# sm = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# # apply SMOTE to generate new samples\n",
    "# X_resampled, y_resampled = sm.fit_resample(X, y)\n",
    "\n",
    "smote = BorderlineSMOTE(random_state=1)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# after applying SMOTE\n",
    "print('After SMOTE:', X_resampled.shape, y_resampled.shape)\n",
    "train_resampled = pd.concat([pd.DataFrame(X_resampled), pd.DataFrame(y_resampled)], axis=1)\n",
    "\n",
    "pd.Series.value_counts(y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After SMOTE technique, the number of participants having heart disease is increased to be equal to the number of non-heart disease records. The SMOTE method balance out the data set so that the number of participants with heart disease equals the number of participants that doesn't, so the accuracy of our prediction will largely increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the data after SMOTE into CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_resampled.to_csv('X_train.csv', index=False)\n",
    "X_test.to_csv('X_test.csv', index=False)\n",
    "y_resampled.to_csv('y_train.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)\n",
    "train_resampled.to_csv('train_data.csv', index=False)\n",
    "test_data.to_csv('test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_resampled)\n",
    "X_train_scale = pd.DataFrame(X_train_scale, columns=X_resampled.columns)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "X_test_scale = pd.DataFrame(X_test_scale, columns=X_resampled.columns)\n",
    "train_data_scale = pd.concat([pd.DataFrame(y_resampled), pd.DataFrame(X_train_scale)], axis=1)\n",
    "test_data_scale = pd.concat([pd.DataFrame(y_test), pd.DataFrame(X_test_scale)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the scaled data into csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train_scale.to_csv('X_train_scale.csv', index=False)\n",
    "X_test_scale.to_csv('X_test_scale.csv', index=False)\n",
    "y_resampled.to_csv('y_train_scale.csv', index=False)\n",
    "y_test.to_csv('y_test_scale.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(data_binary.corr(), cmap='coolwarm', center=0, annot=True)\n",
    "ax.set_title('Correlation Heatmap', fontsize=20)\n",
    "ax.tick_params(labelsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(train_data_scale)\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "PC1 = pca.fit_transform(train_data_scale)[:,0]\n",
    "PC2 = pca.fit_transform(train_data_scale)[:,1]\n",
    "ldngs = pca.components_\n",
    "\n",
    "scalePC1 = 1.0/(PC1.max() - PC1.min())\n",
    "scalePC2 = 1.0/(PC2.max() - PC2.min())\n",
    "features = train_data_scale.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 9))\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    ax.arrow(0, 0, ldngs[0, i], \n",
    "             ldngs[1, i], \n",
    "             head_width=0.03, \n",
    "             head_length=0.03)\n",
    "    ax.text(ldngs[0, i] * 1.15, \n",
    "            ldngs[1, i] * 1.15, \n",
    "            feature, fontsize = 18)\n",
    "\n",
    "ax.set_xlabel('PC1', fontsize=20)\n",
    "ax.set_ylabel('PC2', fontsize=20)\n",
    "ax.set_title('Figure 1', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 9))\n",
    "\n",
    "scatter = ax.scatter(PC1 * scalePC1, \n",
    "                     PC2 * scalePC2, \n",
    "                     c=y_resampled, \n",
    "                     cmap='viridis')\n",
    "\n",
    "ax.set_xlabel('PC1', fontsize=20)\n",
    "ax.set_ylabel('PC2', fontsize=20)\n",
    "ax.set_title('Figure 1', fontsize=20)\n",
    "\n",
    "legend1 = ax.legend(*scatter.legend_elements(),\n",
    "                    loc=\"lower left\", \n",
    "                    title=\"Groups\")\n",
    "ax.add_artist(legend1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 9))\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    ax.arrow(0, 0, ldngs[0, i], \n",
    "             ldngs[1, i], \n",
    "             head_width=0.03, \n",
    "             head_length=0.03)\n",
    "    ax.text(ldngs[0, i] * 1.15, \n",
    "            ldngs[1, i] * 1.15, \n",
    "            feature, fontsize = 18)\n",
    "\n",
    "scatter = ax.scatter(PC1 * scalePC1, \n",
    "                     PC2 * scalePC2, \n",
    "                     c=y_resampled, \n",
    "                     cmap='viridis')\n",
    "\n",
    "ax.set_xlabel('PC1', fontsize=20)\n",
    "ax.set_ylabel('PC2', fontsize=20)\n",
    "ax.set_title('Figure 1', fontsize=20)\n",
    "\n",
    "legend1 = ax.legend(*scatter.legend_elements(),\n",
    "                    loc=\"lower left\", \n",
    "                    title=\"Groups\")\n",
    "ax.add_artist(legend1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,9))\n",
    " \n",
    "for i, feature in enumerate(features):\n",
    "    plt.arrow(0, 0, ldngs[0, i], \n",
    "             ldngs[1, i], \n",
    "              head_width=0.03, \n",
    "             head_length=0.03)\n",
    "    plt.text(ldngs[0, i] * 1.15, \n",
    "            ldngs[1, i] * 1.15, \n",
    "            feature, fontsize=18)\n",
    " \n",
    "sns.scatterplot(x=PC1 * scalePC1,\n",
    "                y=PC2 * scalePC2, \n",
    "                hue=y_resampled,\n",
    "                palette=\"viridis\")\n",
    " \n",
    "plt.xlabel('PC1', fontsize=20)\n",
    "plt.ylabel('PC2', fontsize=20)\n",
    "plt.title('Figure 2', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, 'bo-', linewidth=2)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the loadings for each variable on each principal component\n",
    "loadings = pca.components_.T[:, :5]\n",
    "\n",
    "# Create a dataframe to store the loadings\n",
    "pc_table = pd.DataFrame(loadings, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5'], index=X.columns)\n",
    "\n",
    "# Display the PC table\n",
    "print(pc_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg = Lasso(alpha=0.1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = lasso_reg.predict(X_test)\n",
    "\n",
    "# Compute the mean squared error of the predictions\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Print the coefficients of the model\n",
    "print(\"Coefficients:\", lasso_reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4, 1, 50)\n",
    "\n",
    "# Initialize a list to store the coefficients for each alpha value\n",
    "coefs = []\n",
    "\n",
    "# Fit the model for each alpha value and store the coefficients\n",
    "for a in alphas:\n",
    "    lasso_reg = Lasso(alpha=a)\n",
    "    lasso_reg.fit(X_train, y_train)\n",
    "    coefs.append(lasso_reg.coef_)\n",
    "    \n",
    "# Get the names or labels of the features\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame to store the coefficients\n",
    "df = pd.DataFrame(coefs, columns=feature_names)\n",
    "\n",
    "# Transpose the DataFrame to have the alpha values as the index\n",
    "df = df.transpose()\n",
    "\n",
    "# Print the resulting table\n",
    "print(df)\n",
    "\n",
    "# Plot the coefficients as a function of alpha\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = plt.gca()\n",
    "\n",
    "# Plot the coefficients for each feature\n",
    "for i, c in enumerate(coefs[0]):\n",
    "    ax.plot(alphas, [coef[i] for coef in coefs], label=feature_names[i])\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('alpha')\n",
    "ax.set_ylabel('coefficients')\n",
    "ax.set_title('L1 Regularization Path')\n",
    "\n",
    "# Add a legend to the plot\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.svm import SVC\n",
    "# from scipy.stats import uniform\n",
    "\n",
    "# # assuming X_train and y_train are your training data\n",
    "# # assuming param_distributions is a dictionary of hyperparameters to search over\n",
    "# # assuming scoring_metric is the metric to use for evaluation (e.g. accuracy, f1-score)\n",
    "# # assuming cv is the number of cross-validation folds to use\n",
    "# model = SVC() \n",
    "# param_distributions = {'C': np.logspace(-3, 3, 7),\n",
    "#                        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "#                        'gamma': np.logspace(-3, 3, 7)}\n",
    "# scoring_metric = 'accuracy'\n",
    "# cv = 5\n",
    "\n",
    "# random_search = RandomizedSearchCV(estimator=model, param_distributions=param_distributions, \n",
    "#                                    scoring=scoring_metric, cv=cv, n_iter=10, random_state=42)\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# print('Best hyperparameters:', random_search.best_params_)\n",
    "# print('Best', scoring_metric, 'score:', random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.1, 1, 10, 100],\n",
    "              'penalty': ['l1', 'l2']}\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5)\n",
    "grid_search.fit(X_train_scale, y_resampled)\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_scale, y_resampled)\n",
    "y_pred = model.predict(X_test_scale)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision:\", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "param_grid = {'hidden_layer_sizes': [(50,), (100,), (50,50), (100,50)],\n",
    "              'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "              'activation': ['logistic']}\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5)\n",
    "grid_search.fit(X_train_scale, y_resampled)\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
