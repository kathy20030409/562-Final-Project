{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heart disease is a leading cause of death worldwide, and early detection and prevention can save lives. In this project, we analysed data from a 2020 annual CDC survey data of 400k adults related to their health status, and applied different machine learning algorithms aiming to predict chance of one individual having heart disease. The project first performed data cleaning and feature selection to prepare the data for modeling. Then, several machine learning models such as LDA, Random Forest, SVM, Decision Tree, XGBoost, and Neural Network were applied to find the model with the highest sensitivity for the final prediction. The results showed that the XGBoost model performed the best in terms of sensitivity, and thus was selected as the final model for the prediction of heart disease. We created a user interface and linked with our XGBoost model, which allow user to input data about their personal health status and returns whether the patient require further diagnoise or not. This project highlights the importance of data preparation and model selection in achieving accurate results in the field of medical diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heart disease continues to be a major global health challenge, accounting for significant morbidity and mortality rates. In the United States, nearly half of the population is affected by at least one of the three primary risk factors for heart disease: high blood pressure, high cholesterol, and smoking. Additional indicators, such as diabetic status, obesity (high BMI), inadequate physical activity, and excessive alcohol consumption, further contribute to the complexity of heart disease management. In recent years, advancements in computational power and the proliferation of large datasets have facilitated the application of machine learning techniques in healthcare. These techniques can process vast amounts of data to detect patterns and predict health outcomes, thus offering valuable insights for early identification and timely intervention of heart disease. With the potential to revolutionize clinical decision-making and patient care, machine learning applications in heart disease prediction are increasingly gaining traction among researchers and healthcare professionals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The motivation behind this study lies in the pressing need to improve early detection and prevention of heart disease, which can significantly reduce the associated health burden and improve patient outcomes. By harnessing the power of machine learning and utilizing the rich information available in large-scale health datasets, we aim to develop an effective predictive model that can identify individuals at high risk for heart disease. Such a model would enable healthcare providers to implement targeted interventions, promote healthier lifestyle choices, and facilitate more accurate decision-making in clinical settings. Furthermore, the comparative analysis of various classification algorithms in this study will provide valuable insights into the effectiveness of different machine learning techniques in heart disease prediction. Ultimately, the development of an accurate and robust predictive model has the potential to transform the landscape of heart disease management, enhancing the overall quality of healthcare and potentially saving countless lives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data gathered 401,958 participant data through phone call surveys about their health conditions that may potentially impact heart condition. We interpreted the data by finding unique values of each variables. Variables with only 2 unique values are binary values and we encoded them to 1 and 0s. For categorical variables like age and race, we used one-hot coding method and turned categories into dummy binary variables. For example, original column \"race\" is encoded into breakdown categories like \"Race_Asian\", \"Race_Black\", etc. We randomly chose selected 20% of the dataset to prevent from any kind of modification, as to retain original data for higher credibility. We then introduced Synthetic Minority Over-sampling Technique (SMOTE) to address the problem we have for imbalanced dataset. It selects minority class samples and computes their k-nearest neighbors. Then, by interpolates between a chosen minority class sample and its nearest neighbors, new synthetic samples are added to original dataset. To deal with problem of overfitting, we chosen the specific borderline SMOTE technique, which only select samples that lies on the borderline between minority and majority classes. Thus reduces effects caused by noisy data and resolves overfitting issue. After balancing the dataset, we scaled the dataset and fit the test data to the remaining data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HeartDisease</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>AlcoholDrinking</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>PhysicalHealth</th>\n",
       "      <th>MentalHealth</th>\n",
       "      <th>DiffWalking</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Diabetic</th>\n",
       "      <th>...</th>\n",
       "      <th>Race_Asian</th>\n",
       "      <th>Race_Black</th>\n",
       "      <th>Race_Hispanic</th>\n",
       "      <th>Race_Other</th>\n",
       "      <th>Race_White</th>\n",
       "      <th>GenHealth_Excellent</th>\n",
       "      <th>GenHealth_Fair</th>\n",
       "      <th>GenHealth_Good</th>\n",
       "      <th>GenHealth_Poor</th>\n",
       "      <th>GenHealth_Very good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>16.60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20.34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>26.58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>24.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>23.71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HeartDisease    BMI  Smoking  AlcoholDrinking  Stroke  PhysicalHealth  \\\n",
       "0             0  16.60        1                0       0             3.0   \n",
       "1             0  20.34        0                0       1             0.0   \n",
       "2             0  26.58        1                0       0            20.0   \n",
       "3             0  24.21        0                0       0             0.0   \n",
       "4             0  23.71        0                0       0            28.0   \n",
       "\n",
       "   MentalHealth  DiffWalking  Sex  Diabetic  ...  Race_Asian  Race_Black  \\\n",
       "0          30.0            0    0         1  ...           0           0   \n",
       "1           0.0            0    0         0  ...           0           0   \n",
       "2          30.0            0    1         1  ...           0           0   \n",
       "3           0.0            0    0         0  ...           0           0   \n",
       "4           0.0            1    0         0  ...           0           0   \n",
       "\n",
       "   Race_Hispanic  Race_Other  Race_White  GenHealth_Excellent  GenHealth_Fair  \\\n",
       "0              0           0           1                    0               0   \n",
       "1              0           0           1                    0               0   \n",
       "2              0           0           1                    0               1   \n",
       "3              0           0           1                    0               0   \n",
       "4              0           0           1                    0               0   \n",
       "\n",
       "   GenHealth_Good  GenHealth_Poor  GenHealth_Very good  \n",
       "0               0               0                    1  \n",
       "1               0               0                    1  \n",
       "2               0               0                    0  \n",
       "3               1               0                    0  \n",
       "4               0               0                    1  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"heart_2020_cleaned.csv\")\n",
    "data_binary = data[data.columns].replace({'Yes':1, 'No':0, 'Male':1,'Female':0,'No, borderline diabetes':'0','Yes (during pregnancy)':'1' })\n",
    "data_binary['Diabetic'] = data_binary['Diabetic'].astype(int)\n",
    "categorical_columns = [name for name in data_binary.columns \n",
    "                       if data_binary[name].dtype=='O']\n",
    "data_dummy = pd.get_dummies(data=data_binary, columns=categorical_columns, drop_first=False)\n",
    "data_dummy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (/Users/yutongwu/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m train_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mDataFrame(X_train), pd\u001b[38;5;241m.\u001b[39mDataFrame(y_train)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m test_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mDataFrame(X_test), pd\u001b[38;5;241m.\u001b[39mDataFrame(y_test)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BorderlineSMOTE\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_classification\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/__init__.py:52\u001b[0m\n\u001b[1;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m         combine,\n\u001b[1;32m     54\u001b[0m         ensemble,\n\u001b[1;32m     55\u001b[0m         exceptions,\n\u001b[1;32m     56\u001b[0m         metrics,\n\u001b[1;32m     57\u001b[0m         over_sampling,\n\u001b[1;32m     58\u001b[0m         pipeline,\n\u001b[1;32m     59\u001b[0m         tensorflow,\n\u001b[1;32m     60\u001b[0m         under_sampling,\n\u001b[1;32m     61\u001b[0m         utils,\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/combine/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/combine/_smote_enn.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/base.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_sampling_strategy, check_target_type\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArraysTransformer\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSamplerMixin\u001b[39;00m(BaseEstimator, metaclass\u001b[38;5;241m=\u001b[39mABCMeta):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/utils/_param_validation.py:908\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_valid_param  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m--> 908\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    909\u001b[0m     HasMethods,\n\u001b[1;32m    910\u001b[0m     Hidden,\n\u001b[1;32m    911\u001b[0m     Interval,\n\u001b[1;32m    912\u001b[0m     Options,\n\u001b[1;32m    913\u001b[0m     StrOptions,\n\u001b[1;32m    914\u001b[0m     _ArrayLikes,\n\u001b[1;32m    915\u001b[0m     _Booleans,\n\u001b[1;32m    916\u001b[0m     _Callables,\n\u001b[1;32m    917\u001b[0m     _CVObjects,\n\u001b[1;32m    918\u001b[0m     _InstancesOf,\n\u001b[1;32m    919\u001b[0m     _IterablesNotString,\n\u001b[1;32m    920\u001b[0m     _MissingValues,\n\u001b[1;32m    921\u001b[0m     _NoneConstraint,\n\u001b[1;32m    922\u001b[0m     _PandasNAConstraint,\n\u001b[1;32m    923\u001b[0m     _RandomStates,\n\u001b[1;32m    924\u001b[0m     _SparseMatrices,\n\u001b[1;32m    925\u001b[0m     _VerboseHelper,\n\u001b[1;32m    926\u001b[0m     make_constraint,\n\u001b[1;32m    927\u001b[0m     validate_params,\n\u001b[1;32m    928\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (/Users/yutongwu/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# assume df is the DataFrame and y is the target variable\n",
    "# split the data into training and test sets with 80% for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_dummy.drop(columns=[data_dummy.columns[0]]), \n",
    "                                                    data_dummy.iloc[:,0], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "train_data = pd.concat([pd.DataFrame(X_train), pd.DataFrame(y_train)], axis=1)\n",
    "test_data = pd.concat([pd.DataFrame(X_test), pd.DataFrame(y_test)], axis=1)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X = X_train\n",
    "y = y_train\n",
    "\n",
    "print('Before SMOTE:', X.shape, y.shape)\n",
    "\n",
    "\n",
    "# # create SMOTE object with desired sampling strategy\n",
    "# sm = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# # apply SMOTE to generate new samples\n",
    "# X_resampled, y_resampled = sm.fit_resample(X, y)\n",
    "\n",
    "smote = BorderlineSMOTE(random_state=1)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# after applying SMOTE\n",
    "train_resampled = pd.concat([pd.DataFrame(X_resampled), pd.DataFrame(y_resampled)], axis=1)\n",
    "\n",
    "pd.Series.value_counts(y_resampled)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_resampled)\n",
    "X_train_scale = pd.DataFrame(X_train_scale, columns=X_resampled.columns)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "X_test_scale = pd.DataFrame(X_test_scale, columns=X_resampled.columns)\n",
    "train_data_scale = pd.concat([pd.DataFrame(y_resampled), pd.DataFrame(X_train_scale)], axis=1)\n",
    "test_data_scale = pd.concat([pd.DataFrame(y_test), pd.DataFrame(X_test_scale)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present the application of a correlation heatmap as a valuable tool for feature selection. The primary goal of employing a correlation heatmap is to visualize the pairwise relationships between the features in our dataset. This visualization allows us to identify highly correlated variables, which can often lead to multicollinearity issues in our model, thus negatively affecting its performance.we can easily spot highly correlated features, facilitating the decision-making process for feature selection. Consequently, we can remove or combine redundant features, improving the efficiency of our model and reducing the risk of overfitting. In our dataset, none of the variables are highly correlated. All the correlation values, including the correlation with our fecture variables, are below 0.3, indicating a weak relation. Thus we kept all the features, and hypothesized that none of the variables would have significant impact on the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(data_binary.corr(), cmap='coolwarm', center=0, annot=True)\n",
    "ax.set_title('Correlation Heatmap', fontsize=20)\n",
    "ax.tick_params(labelsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_train_scale)\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "PC1 = pca.fit_transform(X_train_scale)[:,0]\n",
    "PC2 = pca.fit_transform(X_train_scale)[:,1]\n",
    "ldngs = pca.components_\n",
    "\n",
    "scalePC1 = 1.0/(PC1.max() - PC1.min())\n",
    "scalePC2 = 1.0/(PC2.max() - PC2.min())\n",
    "features = X_train.columns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 9))\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    ax.arrow(0, 0, ldngs[0, i], \n",
    "             ldngs[1, i], \n",
    "             head_width=0.03, \n",
    "             head_length=0.03)\n",
    "    ax.text(ldngs[0, i] * 1.15, \n",
    "            ldngs[1, i] * 1.15, \n",
    "            feature, fontsize = 18)\n",
    "\n",
    "scatter = ax.scatter(PC1 * scalePC1, \n",
    "                     PC2 * scalePC2, \n",
    "                     c=y_resampled, \n",
    "                     cmap='viridis')\n",
    "\n",
    "ax.set_xlabel('PC1', fontsize=20)\n",
    "ax.set_ylabel('PC2', fontsize=20)\n",
    "ax.set_title('Figure 1', fontsize=20)\n",
    "\n",
    "legend1 = ax.legend(*scatter.legend_elements(),\n",
    "                    loc=\"lower left\", \n",
    "                    title=\"Groups\")\n",
    "ax.add_artist(legend1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then utlized PCA to analyse the given data. The results from 1 and 0 both gave a wide spread, the variables have large amount of variance. It shows that \"Race_White\" and \"Physical Activity\" has longest vectors, indicating a strong influence on the principal components. And we tend to pay more attention on these two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statistics import mean\n",
    "\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = lasso_reg.predict(X_test)\n",
    "\n",
    "# Compute the mean squared error of the predictions\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "alphas = np.logspace(-4, 1, 50)\n",
    "\n",
    "# Initialize a list to store the coefficients for each alpha value\n",
    "coefs = []\n",
    "\n",
    "# Fit the model for each alpha value and store the coefficients\n",
    "for a in alphas:\n",
    "    lasso_reg = Lasso(alpha=a)\n",
    "    lasso_reg.fit(X_train, y_train)\n",
    "    coefs.append(lasso_reg.coef_)\n",
    "    \n",
    "# Get the names or labels of the features\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame to store the coefficients\n",
    "df = pd.DataFrame(coefs, columns=feature_names)\n",
    "\n",
    "# Transpose the DataFrame to have the alpha values as the index\n",
    "df = df.transpose()\n",
    "\n",
    "# Plot the coefficients as a function of alpha\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = plt.gca()\n",
    "\n",
    "# Plot the coefficients for each feature\n",
    "for i, c in enumerate(coefs[0]):\n",
    "    ax.plot(alphas, [coef[i] for coef in coefs], label=feature_names[i])\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('alpha')\n",
    "ax.set_ylabel('coefficients')\n",
    "ax.set_title('L1 Regularization Path')\n",
    "\n",
    "# Add a legend to the plot\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
